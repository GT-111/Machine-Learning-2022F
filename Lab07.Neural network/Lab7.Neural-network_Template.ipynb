{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 Assignment\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (20 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 12)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "df = pd.read_csv(\"datasets/digit01.csv\", header=None)\n",
    "df.head()\n",
    "\n",
    "y = df[12]\n",
    "X = df.drop(12, axis=1)\n",
    "X.shape\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting dataset into 80% Training and 20% Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.8, shuffle=True)\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a LogisticRegression subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LogisticRegression subclass of nn. Module.\n",
    "########### Write Your Code Here ###########\n",
    "import  torch \n",
    "\n",
    "X_train = torch.from_numpy(X_train.values).float()\n",
    "X_test = torch.from_numpy(X_test.values).float()\n",
    "y_train = torch.from_numpy(y_train.values).long()\n",
    "y_test = torch.from_numpy(y_test.values).long()\n",
    "############################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class LogisticRegression (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(12, 2))\n",
    "    def forward(self, input):\n",
    "        x = input.view([-1, 12])\n",
    "        x = self.net(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim= -1)\n",
    "############################################\n",
    "\n",
    "model_logisticRegession = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "def cross_entropy_loss(output, y):\n",
    "    return F.nll_loss(output, y)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from torch.optim import SGD\n",
    "optimizer_SGD = SGD(model_logisticRegession.parameters(), lr=0.001, momentum=0.9,\n",
    "                         dampening=0.5, weight_decay=0.01, nesterov=False)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss : 0.751581\n",
      "Epoch: 100, loss : 0.485726\n",
      "Epoch: 200, loss : 0.382759\n",
      "Epoch: 300, loss : 0.320889\n",
      "Epoch: 400, loss : 0.278021\n",
      "Epoch: 500, loss : 0.246609\n",
      "Epoch: 600, loss : 0.222672\n",
      "Epoch: 700, loss : 0.203859\n",
      "Epoch: 800, loss : 0.188693\n",
      "Epoch: 900, loss : 0.176207\n",
      "Epoch: 1000, loss : 0.165742\n",
      "Epoch: 1100, loss : 0.156840\n",
      "Epoch: 1200, loss : 0.149168\n",
      "Epoch: 1300, loss : 0.142481\n",
      "Epoch: 1400, loss : 0.136595\n",
      "Epoch: 1500, loss : 0.131369\n",
      "Epoch: 1600, loss : 0.126695\n",
      "Epoch: 1700, loss : 0.122484\n",
      "Epoch: 1800, loss : 0.118668\n",
      "Epoch: 1900, loss : 0.115191\n",
      "Epoch: 2000, loss : 0.112006\n",
      "Epoch: 2100, loss : 0.109077\n",
      "Epoch: 2200, loss : 0.106372\n",
      "Epoch: 2300, loss : 0.103864\n",
      "Epoch: 2400, loss : 0.101531\n",
      "Epoch: 2500, loss : 0.099355\n",
      "Epoch: 2600, loss : 0.097318\n",
      "Epoch: 2700, loss : 0.095407\n",
      "Epoch: 2800, loss : 0.093610\n",
      "Epoch: 2900, loss : 0.091916\n",
      "Epoch: 3000, loss : 0.090315\n",
      "Epoch: 3100, loss : 0.088801\n",
      "Epoch: 3200, loss : 0.087365\n",
      "Epoch: 3300, loss : 0.086002\n",
      "Epoch: 3400, loss : 0.084705\n",
      "Epoch: 3500, loss : 0.083470\n",
      "Epoch: 3600, loss : 0.082291\n",
      "Epoch: 3700, loss : 0.081166\n",
      "Epoch: 3800, loss : 0.080090\n",
      "Epoch: 3900, loss : 0.079060\n",
      "Epoch: 4000, loss : 0.078073\n",
      "Epoch: 4100, loss : 0.077126\n",
      "Epoch: 4200, loss : 0.076217\n",
      "Epoch: 4300, loss : 0.075343\n",
      "Epoch: 4400, loss : 0.074503\n",
      "Epoch: 4500, loss : 0.073694\n",
      "Epoch: 4600, loss : 0.072915\n",
      "Epoch: 4700, loss : 0.072164\n",
      "Epoch: 4800, loss : 0.071440\n",
      "Epoch: 4900, loss : 0.070741\n",
      "Epoch: 5000, loss : 0.070065\n",
      "Epoch: 5100, loss : 0.069413\n",
      "Epoch: 5200, loss : 0.068782\n",
      "Epoch: 5300, loss : 0.068171\n",
      "Epoch: 5400, loss : 0.067580\n",
      "Epoch: 5500, loss : 0.067008\n",
      "Epoch: 5600, loss : 0.066454\n",
      "Epoch: 5700, loss : 0.065916\n",
      "Epoch: 5800, loss : 0.065395\n",
      "Epoch: 5900, loss : 0.064889\n",
      "Epoch: 6000, loss : 0.064399\n",
      "Epoch: 6100, loss : 0.063923\n",
      "Epoch: 6200, loss : 0.063460\n",
      "Epoch: 6300, loss : 0.063011\n",
      "Epoch: 6400, loss : 0.062574\n",
      "Epoch: 6500, loss : 0.062149\n",
      "Epoch: 6600, loss : 0.061736\n",
      "Epoch: 6700, loss : 0.061335\n",
      "Epoch: 6800, loss : 0.060944\n",
      "Epoch: 6900, loss : 0.060564\n",
      "Epoch: 7000, loss : 0.060193\n",
      "Epoch: 7100, loss : 0.059833\n",
      "Epoch: 7200, loss : 0.059481\n",
      "Epoch: 7300, loss : 0.059139\n",
      "Epoch: 7400, loss : 0.058806\n",
      "Epoch: 7500, loss : 0.058480\n",
      "Epoch: 7600, loss : 0.058163\n",
      "Epoch: 7700, loss : 0.057854\n",
      "Epoch: 7800, loss : 0.057553\n",
      "Epoch: 7900, loss : 0.057259\n",
      "Epoch: 8000, loss : 0.056971\n",
      "Epoch: 8100, loss : 0.056691\n",
      "Epoch: 8200, loss : 0.056418\n",
      "Epoch: 8300, loss : 0.056151\n",
      "Epoch: 8400, loss : 0.055890\n",
      "Epoch: 8500, loss : 0.055635\n",
      "Epoch: 8600, loss : 0.055387\n",
      "Epoch: 8700, loss : 0.055144\n",
      "Epoch: 8800, loss : 0.054906\n",
      "Epoch: 8900, loss : 0.054674\n",
      "Epoch: 9000, loss : 0.054447\n",
      "Epoch: 9100, loss : 0.054225\n",
      "Epoch: 9200, loss : 0.054008\n",
      "Epoch: 9300, loss : 0.053796\n",
      "Epoch: 9400, loss : 0.053589\n",
      "Epoch: 9500, loss : 0.053386\n",
      "Epoch: 9600, loss : 0.053187\n",
      "Epoch: 9700, loss : 0.052993\n",
      "Epoch: 9800, loss : 0.052803\n",
      "Epoch: 9900, loss : 0.052617\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def train_logistic(epoch, model:nn.Module, optimizer, X_train, y_train) :\n",
    "    model.train()\n",
    "    for _ in range(epoch):\n",
    "        \n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "            X_train, y_train = X_train.cuda(), y_train.cuda()\n",
    "\n",
    "            output = model(X_train)\n",
    "            loss = cross_entropy_loss(output, y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if _ % 100 == 0:\n",
    "                #save model\n",
    "            \n",
    "                print('Epoch: %d, loss : %lf' %(_, loss.item()))\n",
    "\n",
    "EPOCH = 10000\n",
    "train_logistic(EPOCH, model=model_logisticRegession, optimizer= optimizer_SGD, X_train = X_train, y_train = y_train)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average ACC:  1.0\n",
      "average loss:  0.04008827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import numpy as np\n",
    "def test(model:nn.Module, X_test, y_test):\n",
    "    model.eval()\n",
    "    ACC_list = []\n",
    "    loss_list = []\n",
    "    pred_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if use_cuda:\n",
    "            model = model.cuda()\n",
    "            X_test, y_test = X_test.cuda(), y_test.cuda()\n",
    "        output = model(X_test)\n",
    "        loss = cross_entropy_loss(output, y_test)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        ACC = pred.eq(y_test.view_as(pred)).float().mean()\n",
    "            \n",
    "        ACC_list.append(ACC.cpu())\n",
    "        loss_list.append(loss.cpu())\n",
    "        pred_list.extend(pred.cpu().flatten())\n",
    "\n",
    "    print('average ACC: ',np.mean(ACC_list))\n",
    "    print('average loss: ',np.mean(loss_list))\n",
    "    return pred_list\n",
    "test(model=model_logisticRegession, X_test=X_test,y_test=y_test)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average ACC:  1.0\n",
      "average loss:  0.04008827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         6\n",
      "           1       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        13\n",
      "   macro avg       1.00      1.00      1.00        13\n",
      "weighted avg       1.00      1.00      1.00        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "y_predicted_cls = test(model=model_logisticRegession, X_test=X_test,y_test=y_test)\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:0.13066047430038452\n",
      "std:0.30810782313346863\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "device = torch.device(\"cuda:0\") \n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "batch_size=60000\n",
    "transform=Compose([ToTensor()])\n",
    "train_dataset=MNIST(root='datasets/',train=True,download=False,transform=transform)\n",
    "train_loader=DataLoader(train_dataset,shuffle=True,batch_size=batch_size)\n",
    "\n",
    "test_dataset=MNIST(root='datasets/',train=False,download=False,transform=transform)\n",
    "test_loader=DataLoader(test_dataset,shuffle=False,batch_size=batch_size)\n",
    "\n",
    "for batch_idx, data in enumerate(train_loader, 0):\n",
    "    inputs, targets = data \n",
    "    x=inputs.view(-1,28*28) \n",
    "    x_std=x.std().item() \n",
    "    x_mean=x.mean().item() \n",
    "\n",
    "print('mean:'+str(x_mean))\n",
    "print('std:'+str(x_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = Compose(\n",
    "[ToTensor(), Normalize(mean=(0.1307), std=(0.3081)).to(device=device)]    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "batch_size=60000\n",
    "train_dataset = MNIST(root='datasets/',train=True,download=False, transform=transform)\n",
    "train_loader=DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset=MNIST(root='datasets/',train=False, download=False, transform=transform)\n",
    "test_loader=DataLoader(test_dataset,shuffle=False,batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "############################################                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a MLP subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "#input size [BATCH_SIZE, 28, 28]\n",
    "\n",
    "class MnistModel (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(28 * 28,200),nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(200,100),nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Linear(100,20),nn.ReLU())\n",
    "        self.layer4 = nn.Linear(20,10)\n",
    "\n",
    "        self.net = nn.Sequential(self.layer1, self.layer2, self.layer3, self.layer4)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if use_cuda:\n",
    "            input = input.cuda()\n",
    "        x = input.view([-1, 28 * 28])\n",
    "        x = self.net(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim= -1)\n",
    "############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import torch\n",
    "import os\n",
    "model = MnistModel().to(device)\n",
    "if os.path.exists('./model/model.pkl'):\n",
    "    model.load_state_dict(state_dict= torch.load('./model/model.pkl'))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "def cross_entropy_loss(output, y):\n",
    "    return F.nll_loss(output, y)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from torch.optim import Adam\n",
    "optimizer_Adam = Adam(model.parameters(), lr=0.0001)\n",
    "if os.path.exists('./model/optimizer.pkl'):\n",
    "    optimizer_Adam.load_state_dict(state_dict= torch.load('./model/optimizer.pkl'))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, idx: 0, loss : 0.000008\n",
      "Epoch: 0, idx: 100, loss : 0.000004\n",
      "Epoch: 0, idx: 200, loss : 0.000006\n",
      "Epoch: 0, idx: 300, loss : 0.000003\n",
      "Epoch: 0, idx: 400, loss : 0.000002\n",
      "Epoch: 1, idx: 0, loss : 0.000005\n",
      "Epoch: 1, idx: 100, loss : 0.000001\n",
      "Epoch: 1, idx: 200, loss : 0.000001\n",
      "Epoch: 1, idx: 300, loss : 0.000002\n",
      "Epoch: 1, idx: 400, loss : 0.000001\n",
      "Epoch: 2, idx: 0, loss : 0.000001\n",
      "Epoch: 2, idx: 100, loss : 0.000003\n",
      "Epoch: 2, idx: 200, loss : 0.000003\n",
      "Epoch: 2, idx: 300, loss : 0.000001\n",
      "Epoch: 2, idx: 400, loss : 0.000002\n",
      "Epoch: 3, idx: 0, loss : 0.000006\n",
      "Epoch: 3, idx: 100, loss : 0.000005\n",
      "Epoch: 3, idx: 200, loss : 0.000002\n",
      "Epoch: 3, idx: 300, loss : 0.000007\n",
      "Epoch: 3, idx: 400, loss : 0.000002\n",
      "Epoch: 4, idx: 0, loss : 0.000004\n",
      "Epoch: 4, idx: 100, loss : 0.000001\n",
      "Epoch: 4, idx: 200, loss : 0.000002\n",
      "Epoch: 4, idx: 300, loss : 0.000006\n",
      "Epoch: 4, idx: 400, loss : 0.000003\n",
      "Epoch: 5, idx: 0, loss : 0.000004\n",
      "Epoch: 5, idx: 100, loss : 0.000002\n",
      "Epoch: 5, idx: 200, loss : 0.000001\n",
      "Epoch: 5, idx: 300, loss : 0.000004\n",
      "Epoch: 5, idx: 400, loss : 0.000007\n",
      "Epoch: 6, idx: 0, loss : 0.000002\n",
      "Epoch: 6, idx: 100, loss : 0.000001\n",
      "Epoch: 6, idx: 200, loss : 0.000006\n",
      "Epoch: 6, idx: 300, loss : 0.000003\n",
      "Epoch: 6, idx: 400, loss : 0.000002\n",
      "Epoch: 7, idx: 0, loss : 0.000004\n",
      "Epoch: 7, idx: 100, loss : 0.000001\n",
      "Epoch: 7, idx: 200, loss : 0.000002\n",
      "Epoch: 7, idx: 300, loss : 0.000002\n",
      "Epoch: 7, idx: 400, loss : 0.000003\n",
      "Epoch: 8, idx: 0, loss : 0.000001\n",
      "Epoch: 8, idx: 100, loss : 0.000001\n",
      "Epoch: 8, idx: 200, loss : 0.000002\n",
      "Epoch: 8, idx: 300, loss : 0.000004\n",
      "Epoch: 8, idx: 400, loss : 0.000002\n",
      "Epoch: 9, idx: 0, loss : 0.000001\n",
      "Epoch: 9, idx: 100, loss : 0.000001\n",
      "Epoch: 9, idx: 200, loss : 0.000003\n",
      "Epoch: 9, idx: 300, loss : 0.000004\n",
      "Epoch: 9, idx: 400, loss : 0.000001\n",
      "Epoch: 10, idx: 0, loss : 0.000002\n",
      "Epoch: 10, idx: 100, loss : 0.000001\n",
      "Epoch: 10, idx: 200, loss : 0.000001\n",
      "Epoch: 10, idx: 300, loss : 0.000001\n",
      "Epoch: 10, idx: 400, loss : 0.001861\n",
      "Epoch: 11, idx: 0, loss : 0.000003\n",
      "Epoch: 11, idx: 100, loss : 0.000147\n",
      "Epoch: 11, idx: 200, loss : 0.000025\n",
      "Epoch: 11, idx: 300, loss : 0.000008\n",
      "Epoch: 11, idx: 400, loss : 0.000005\n",
      "Epoch: 12, idx: 0, loss : 0.000025\n",
      "Epoch: 12, idx: 100, loss : 0.000113\n",
      "Epoch: 12, idx: 200, loss : 0.000041\n",
      "Epoch: 12, idx: 300, loss : 0.000287\n",
      "Epoch: 12, idx: 400, loss : 0.000013\n",
      "Epoch: 13, idx: 0, loss : 0.000001\n",
      "Epoch: 13, idx: 100, loss : 0.000009\n",
      "Epoch: 13, idx: 200, loss : 0.000021\n",
      "Epoch: 13, idx: 300, loss : 0.000020\n",
      "Epoch: 13, idx: 400, loss : 0.000044\n",
      "Epoch: 14, idx: 0, loss : 0.000019\n",
      "Epoch: 14, idx: 100, loss : 0.000016\n",
      "Epoch: 14, idx: 200, loss : 0.000016\n",
      "Epoch: 14, idx: 300, loss : 0.000002\n",
      "Epoch: 14, idx: 400, loss : 0.000024\n",
      "Epoch: 15, idx: 0, loss : 0.000009\n",
      "Epoch: 15, idx: 100, loss : 0.000001\n",
      "Epoch: 15, idx: 200, loss : 0.000041\n",
      "Epoch: 15, idx: 300, loss : 0.000008\n",
      "Epoch: 15, idx: 400, loss : 0.000008\n",
      "Epoch: 16, idx: 0, loss : 0.000019\n",
      "Epoch: 16, idx: 100, loss : 0.000008\n",
      "Epoch: 16, idx: 200, loss : 0.000029\n",
      "Epoch: 16, idx: 300, loss : 0.000009\n",
      "Epoch: 16, idx: 400, loss : 0.000010\n",
      "Epoch: 17, idx: 0, loss : 0.000012\n",
      "Epoch: 17, idx: 100, loss : 0.000061\n",
      "Epoch: 17, idx: 200, loss : 0.000012\n",
      "Epoch: 17, idx: 300, loss : 0.000006\n",
      "Epoch: 17, idx: 400, loss : 0.000027\n",
      "Epoch: 18, idx: 0, loss : 0.000042\n",
      "Epoch: 18, idx: 100, loss : 0.000009\n",
      "Epoch: 18, idx: 200, loss : 0.000025\n",
      "Epoch: 18, idx: 300, loss : 0.000007\n",
      "Epoch: 18, idx: 400, loss : 0.000009\n",
      "Epoch: 19, idx: 0, loss : 0.000004\n",
      "Epoch: 19, idx: 100, loss : 0.000005\n",
      "Epoch: 19, idx: 200, loss : 0.000012\n",
      "Epoch: 19, idx: 300, loss : 0.000005\n",
      "Epoch: 19, idx: 400, loss : 0.000009\n",
      "Epoch: 20, idx: 0, loss : 0.000017\n",
      "Epoch: 20, idx: 100, loss : 0.000004\n",
      "Epoch: 20, idx: 200, loss : 0.000007\n",
      "Epoch: 20, idx: 300, loss : 0.000002\n",
      "Epoch: 20, idx: 400, loss : 0.000001\n",
      "Epoch: 21, idx: 0, loss : 0.000005\n",
      "Epoch: 21, idx: 100, loss : 0.000002\n",
      "Epoch: 21, idx: 200, loss : 0.000007\n",
      "Epoch: 21, idx: 300, loss : 0.000007\n",
      "Epoch: 21, idx: 400, loss : 0.000004\n",
      "Epoch: 22, idx: 0, loss : 0.000007\n",
      "Epoch: 22, idx: 100, loss : 0.000005\n",
      "Epoch: 22, idx: 200, loss : 0.000002\n",
      "Epoch: 22, idx: 300, loss : 0.000003\n",
      "Epoch: 22, idx: 400, loss : 0.000003\n",
      "Epoch: 23, idx: 0, loss : 0.000004\n",
      "Epoch: 23, idx: 100, loss : 0.000003\n",
      "Epoch: 23, idx: 200, loss : 0.000011\n",
      "Epoch: 23, idx: 300, loss : 0.000002\n",
      "Epoch: 23, idx: 400, loss : 0.000002\n",
      "Epoch: 24, idx: 0, loss : 0.000009\n",
      "Epoch: 24, idx: 100, loss : 0.000003\n",
      "Epoch: 24, idx: 200, loss : 0.000002\n",
      "Epoch: 24, idx: 300, loss : 0.000005\n",
      "Epoch: 24, idx: 400, loss : 0.000014\n",
      "Epoch: 25, idx: 0, loss : 0.000003\n",
      "Epoch: 25, idx: 100, loss : 0.000002\n",
      "Epoch: 25, idx: 200, loss : 0.000007\n",
      "Epoch: 25, idx: 300, loss : 0.000005\n",
      "Epoch: 25, idx: 400, loss : 0.000003\n",
      "Epoch: 26, idx: 0, loss : 0.000004\n",
      "Epoch: 26, idx: 100, loss : 0.000004\n",
      "Epoch: 26, idx: 200, loss : 0.000004\n",
      "Epoch: 26, idx: 300, loss : 0.000006\n",
      "Epoch: 26, idx: 400, loss : 0.000001\n",
      "Epoch: 27, idx: 0, loss : 0.000000\n",
      "Epoch: 27, idx: 100, loss : 0.000001\n",
      "Epoch: 27, idx: 200, loss : 0.000005\n",
      "Epoch: 27, idx: 300, loss : 0.000005\n",
      "Epoch: 27, idx: 400, loss : 0.000002\n",
      "Epoch: 28, idx: 0, loss : 0.000001\n",
      "Epoch: 28, idx: 100, loss : 0.000004\n",
      "Epoch: 28, idx: 200, loss : 0.000002\n",
      "Epoch: 28, idx: 300, loss : 0.000001\n",
      "Epoch: 28, idx: 400, loss : 0.000002\n",
      "Epoch: 29, idx: 0, loss : 0.000003\n",
      "Epoch: 29, idx: 100, loss : 0.000004\n",
      "Epoch: 29, idx: 200, loss : 0.000004\n",
      "Epoch: 29, idx: 300, loss : 0.000002\n",
      "Epoch: 29, idx: 400, loss : 0.000002\n",
      "Epoch: 30, idx: 0, loss : 0.000001\n",
      "Epoch: 30, idx: 100, loss : 0.000002\n",
      "Epoch: 30, idx: 200, loss : 0.000001\n",
      "Epoch: 30, idx: 300, loss : 0.000002\n",
      "Epoch: 30, idx: 400, loss : 0.000006\n",
      "Epoch: 31, idx: 0, loss : 0.000001\n",
      "Epoch: 31, idx: 100, loss : 0.000005\n",
      "Epoch: 31, idx: 200, loss : 0.000002\n",
      "Epoch: 31, idx: 300, loss : 0.000001\n",
      "Epoch: 31, idx: 400, loss : 0.000002\n",
      "Epoch: 32, idx: 0, loss : 0.000003\n",
      "Epoch: 32, idx: 100, loss : 0.000002\n",
      "Epoch: 32, idx: 200, loss : 0.000001\n",
      "Epoch: 32, idx: 300, loss : 0.000000\n",
      "Epoch: 32, idx: 400, loss : 0.000001\n",
      "Epoch: 33, idx: 0, loss : 0.000001\n",
      "Epoch: 33, idx: 100, loss : 0.000001\n",
      "Epoch: 33, idx: 200, loss : 0.000003\n",
      "Epoch: 33, idx: 300, loss : 0.000002\n",
      "Epoch: 33, idx: 400, loss : 0.000002\n",
      "Epoch: 34, idx: 0, loss : 0.000002\n",
      "Epoch: 34, idx: 100, loss : 0.000003\n",
      "Epoch: 34, idx: 200, loss : 0.000001\n",
      "Epoch: 34, idx: 300, loss : 0.000003\n",
      "Epoch: 34, idx: 400, loss : 0.000002\n",
      "Epoch: 35, idx: 0, loss : 0.000001\n",
      "Epoch: 35, idx: 100, loss : 0.000000\n",
      "Epoch: 35, idx: 200, loss : 0.000001\n",
      "Epoch: 35, idx: 300, loss : 0.000002\n",
      "Epoch: 35, idx: 400, loss : 0.000001\n",
      "Epoch: 36, idx: 0, loss : 0.000000\n",
      "Epoch: 36, idx: 100, loss : 0.000001\n",
      "Epoch: 36, idx: 200, loss : 0.000000\n",
      "Epoch: 36, idx: 300, loss : 0.000001\n",
      "Epoch: 36, idx: 400, loss : 0.000001\n",
      "Epoch: 37, idx: 0, loss : 0.000003\n",
      "Epoch: 37, idx: 100, loss : 0.000001\n",
      "Epoch: 37, idx: 200, loss : 0.000001\n",
      "Epoch: 37, idx: 300, loss : 0.000001\n",
      "Epoch: 37, idx: 400, loss : 0.000001\n",
      "Epoch: 38, idx: 0, loss : 0.000002\n",
      "Epoch: 38, idx: 100, loss : 0.000001\n",
      "Epoch: 38, idx: 200, loss : 0.000001\n",
      "Epoch: 38, idx: 300, loss : 0.000001\n",
      "Epoch: 38, idx: 400, loss : 0.000001\n",
      "Epoch: 39, idx: 0, loss : 0.000000\n",
      "Epoch: 39, idx: 100, loss : 0.000000\n",
      "Epoch: 39, idx: 200, loss : 0.000001\n",
      "Epoch: 39, idx: 300, loss : 0.000001\n",
      "Epoch: 39, idx: 400, loss : 0.000000\n",
      "Epoch: 40, idx: 0, loss : 0.000001\n",
      "Epoch: 40, idx: 100, loss : 0.000000\n",
      "Epoch: 40, idx: 200, loss : 0.000001\n",
      "Epoch: 40, idx: 300, loss : 0.000000\n",
      "Epoch: 40, idx: 400, loss : 0.000001\n",
      "Epoch: 41, idx: 0, loss : 0.005203\n",
      "Epoch: 41, idx: 100, loss : 0.004409\n",
      "Epoch: 41, idx: 200, loss : 0.001912\n",
      "Epoch: 41, idx: 300, loss : 0.000007\n",
      "Epoch: 41, idx: 400, loss : 0.000004\n",
      "Epoch: 42, idx: 0, loss : 0.000005\n",
      "Epoch: 42, idx: 100, loss : 0.000007\n",
      "Epoch: 42, idx: 200, loss : 0.000005\n",
      "Epoch: 42, idx: 300, loss : 0.000037\n",
      "Epoch: 42, idx: 400, loss : 0.000015\n",
      "Epoch: 43, idx: 0, loss : 0.000019\n",
      "Epoch: 43, idx: 100, loss : 0.000004\n",
      "Epoch: 43, idx: 200, loss : 0.000011\n",
      "Epoch: 43, idx: 300, loss : 0.000006\n",
      "Epoch: 43, idx: 400, loss : 0.000017\n",
      "Epoch: 44, idx: 0, loss : 0.000053\n",
      "Epoch: 44, idx: 100, loss : 0.000032\n",
      "Epoch: 44, idx: 200, loss : 0.000000\n",
      "Epoch: 44, idx: 300, loss : 0.000002\n",
      "Epoch: 44, idx: 400, loss : 0.000001\n",
      "Epoch: 45, idx: 0, loss : 0.000052\n",
      "Epoch: 45, idx: 100, loss : 0.000008\n",
      "Epoch: 45, idx: 200, loss : 0.000003\n",
      "Epoch: 45, idx: 300, loss : 0.000003\n",
      "Epoch: 45, idx: 400, loss : 0.000021\n",
      "Epoch: 46, idx: 0, loss : 0.000027\n",
      "Epoch: 46, idx: 100, loss : 0.000006\n",
      "Epoch: 46, idx: 200, loss : 0.000012\n",
      "Epoch: 46, idx: 300, loss : 0.000002\n",
      "Epoch: 46, idx: 400, loss : 0.000008\n",
      "Epoch: 47, idx: 0, loss : 0.000006\n",
      "Epoch: 47, idx: 100, loss : 0.000005\n",
      "Epoch: 47, idx: 200, loss : 0.000009\n",
      "Epoch: 47, idx: 300, loss : 0.000014\n",
      "Epoch: 47, idx: 400, loss : 0.000003\n",
      "Epoch: 48, idx: 0, loss : 0.000006\n",
      "Epoch: 48, idx: 100, loss : 0.000001\n",
      "Epoch: 48, idx: 200, loss : 0.000012\n",
      "Epoch: 48, idx: 300, loss : 0.000002\n",
      "Epoch: 48, idx: 400, loss : 0.000017\n",
      "Epoch: 49, idx: 0, loss : 0.000001\n",
      "Epoch: 49, idx: 100, loss : 0.000003\n",
      "Epoch: 49, idx: 200, loss : 0.000002\n",
      "Epoch: 49, idx: 300, loss : 0.000001\n",
      "Epoch: 49, idx: 400, loss : 0.000016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch, model:nn.Module, optimizer, data_loader) :\n",
    "    model.train()\n",
    "    for _ in range(epoch):\n",
    "        for idx, (input, target) in enumerate(data_loader):\n",
    "            if use_cuda:\n",
    "                model = model.cuda()\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            output = model(input)\n",
    "            loss = cross_entropy_loss(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if idx % 100 == 0:\n",
    "                #save model\n",
    "                torch.save(model.state_dict(), './model/model.pkl')\n",
    "                torch.save(optimizer.state_dict(), './model/optimizer.pkl')\n",
    "\n",
    "\n",
    "                print('Epoch: %d, idx: %d, loss : %lf' %(_, idx, loss.item()))\n",
    "\n",
    "EPOCH = 50\n",
    "train(EPOCH, model=model, optimizer= optimizer_Adam, data_loader = train_loader)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average ACC:  0.97992486\n",
      "average loss:  0.1724151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(7),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(8),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(9),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(4),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(6),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(8),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(7),\n",
       " tensor(7),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(5),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(2),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(3),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(5),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(0),\n",
       " tensor(2),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(8),\n",
       " tensor(8),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(5),\n",
       " tensor(5),\n",
       " tensor(8),\n",
       " tensor(6),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(6),\n",
       " tensor(6),\n",
       " tensor(3),\n",
       " tensor(2),\n",
       " tensor(7),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(7),\n",
       " tensor(4),\n",
       " tensor(6),\n",
       " tensor(4),\n",
       " tensor(9),\n",
       " tensor(5),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(3),\n",
       " tensor(4),\n",
       " tensor(7),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(9),\n",
       " tensor(1),\n",
       " tensor(4),\n",
       " tensor(4),\n",
       " tensor(5),\n",
       " tensor(4),\n",
       " tensor(0),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(1),\n",
       " tensor(5),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(3),\n",
       " tensor(8),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(6),\n",
       " tensor(7),\n",
       " tensor(1),\n",
       " tensor(6),\n",
       " tensor(2),\n",
       " tensor(3),\n",
       " tensor(9),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(2),\n",
       " tensor(2),\n",
       " tensor(0),\n",
       " tensor(8),\n",
       " tensor(9),\n",
       " ...]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import numpy as np\n",
    "def test(model:nn.Module, data_loader):\n",
    "    model.eval()\n",
    "    ACC_list = []\n",
    "    loss_list = []\n",
    "    pred_list = []\n",
    "    for idx, (input, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            if use_cuda:\n",
    "                model = model.cuda()\n",
    "                input, target = input.cuda(), target.cuda()\n",
    "            output = model(input)\n",
    "            loss = cross_entropy_loss(output, target)\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            ACC = pred.eq(target.view_as(pred)).float().mean()\n",
    "            \n",
    "            ACC_list.append(ACC.cpu())\n",
    "            loss_list.append(loss.cpu())\n",
    "            pred_list.extend(pred.cpu().flatten())\n",
    "\n",
    "    print('average ACC: ',np.mean(ACC_list))\n",
    "    print('average loss: ',np.mean(loss_list))\n",
    "    return pred_list\n",
    "test(model=model, data_loader=test_loader)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average ACC:  0.97992486\n",
      "average loss:  0.1724151\n",
      "10000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.98      0.98      0.98      1032\n",
      "           3       0.98      0.98      0.98      1010\n",
      "           4       0.98      0.98      0.98       982\n",
      "           5       0.98      0.98      0.98       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.98      0.97      0.98      1028\n",
      "           8       0.97      0.98      0.98       974\n",
      "           9       0.97      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_predicted_cls = test(model=model, data_loader=test_loader)\n",
    "print(len(y_predicted_cls))\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (10 points )\n",
    "\n",
    "#### 1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "1.逻辑回归的loss function 是训练集上的极大似然函数，感知机是误分数据点到分割超平面的距离的和。优化目标不一样，前者是寻找参数最大化似然。后者是寻找参数最小化误分点到分割平面的距离。虽然都是用梯度下降。\n",
    "\n",
    "2.逻辑回归本质上是回归，回归是需要输出一个连续的数值，所以它寻求的是the probability of positive output given an input.\n",
    "\n",
    "#### 2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "优点：\n",
    "\n",
    "1.具有自学习功能。例如实现图像识别时，只在先把许多不同的图像样板和对应的应识别的结果输入人工神经网络，网络就会通过自学习功能，慢慢学会识别类似的图像。\n",
    "\n",
    "2.具有高速寻找优化解的能力。\n",
    "\n",
    "缺点：\n",
    "\n",
    "1.“黑盒”，具有不可解释性\n",
    "\n",
    "2.数据量低时，表现较差\n",
    "\n",
    "#### 3.What is the role of Activation Function in Neural networks?\n",
    "\n",
    "激活函数的作用是去线性化，解决线性模型的表达、分类能力不足的问题。改变之前数据的线性关系，如果网络中全部是线性变换，则多层网络可以通过矩阵变换，直接转换成一层神经网络。若没有激活函数，神经网络只是线性模型的线性组合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91c805c7118eea5ae8a6fe3ed5530ff19891ccda8bc820923824999ed0a29188"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
